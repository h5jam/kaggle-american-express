{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import scipy.stats\n",
    "import warnings\n",
    "import gc\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "\n",
    "INFERENCE = True # set to False if you only want to cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1\n",
      "Computed avg 1\n",
      "Computed last 1\n",
      "Read 0\n",
      "Computed avg 0\n",
      "Computed last 0\n",
      "Shapes: (458913, 197) (458913,) (924621, 197)\n",
      "CPU times: user 32 s, sys: 12.5 s, total: 44.5 s\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_avg = ['B_1', 'B_11', 'B_16', 'B_17', 'B_18', 'B_2', 'B_20',\n",
    "                'B_28', 'B_3', 'B_4', 'B_5', 'B_7', 'B_9', 'D_112',\n",
    "                'D_121', 'D_141', 'D_39', 'D_41', 'D_42', 'D_43',\n",
    "                'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', \n",
    "                'D_50', 'D_51', 'D_53', 'D_54', 'D_56', 'D_58', \n",
    "                'D_59', 'D_60', 'D_91', 'P_2', 'P_3', 'R_1', 'R_2', \n",
    "                'R_27', 'R_3', 'R_7', 'S_11', 'S_26', 'S_3', 'S_5']\n",
    "features_last = ['B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_15', 'B_16',\n",
    "                 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_22', 'B_23',\n",
    "                 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3',\n",
    "                 'B_32', 'B_33', 'B_36', 'B_38', 'B_39', 'B_4', 'B_40',\n",
    "                 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9',\n",
    "                 'D_102', 'D_103', 'D_105', 'D_106', 'D_107', 'D_109',\n",
    "                 'D_112', 'D_115', 'D_117', 'D_118', 'D_119', 'D_120',\n",
    "                 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_127', \n",
    "                 'D_129', 'D_132', 'D_133', 'D_135', 'D_136', 'D_137', \n",
    "                 'D_140', 'D_141', 'D_143', 'D_145', 'D_39', 'D_41',\n",
    "                 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48',\n",
    "                 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55',\n",
    "                 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63',\n",
    "                 'D_64', 'D_66', 'D_70', 'D_72', 'D_73', 'D_74', 'D_75',\n",
    "                 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_82', 'D_83',\n",
    "                 'D_84', 'D_86', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96',\n",
    "                 'P_2', 'P_3', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13',\n",
    "                 'R_14', 'R_15', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', \n",
    "                 'R_21', 'R_22', 'R_24', 'R_25', 'R_26', 'R_27', 'R_3',\n",
    "                 'R_4', 'R_5', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12',\n",
    "                 'S_13', 'S_15', 'S_17', 'S_20', 'S_22', 'S_23', \n",
    "                 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6',\n",
    "                 'S_7', 'S_8', 'S_9']\n",
    "                 \n",
    "\n",
    "train_test = [None, None] # first element is train, second element is test\n",
    "for i in [1, 0] if INFERENCE else [0]:\n",
    "    train_test[i] = pd.read_feather(['dataset/train_data.ftr',\n",
    "                                     'dataset/test_data.ftr'][i])\n",
    "    cid = pd.Categorical(train_test[i].pop('customer_ID'), ordered=True)\n",
    "    last = (cid != np.roll(cid, -1)) # mask for last statement of every customer\n",
    "    if i == 0: # train\n",
    "        target = train_test[0].loc[last, 'target']\n",
    "    gc.collect()\n",
    "    print('Read', i)\n",
    "    df_avg = (train_test[i][features_avg]\n",
    "              .groupby(cid)\n",
    "              .mean()\n",
    "              .rename(columns={f: f\"{f}_avg\" for f in features_avg})\n",
    "             )\n",
    "    gc.collect()\n",
    "    print('Computed avg', i)\n",
    "    train_test[i] = (train_test[i].loc[last, features_last]\n",
    "                     .rename(columns={f: f\"{f}_last\" for f in features_last})\n",
    "                     .set_index(np.asarray(cid[last]))\n",
    "                    )\n",
    "    gc.collect()\n",
    "    print('Computed last', i)\n",
    "    train_test[i] = pd.concat([train_test[i], df_avg], axis=1)\n",
    "    del df_avg, cid, last\n",
    "\n",
    "train, test = tuple(train_test)\n",
    "del train_test\n",
    "if INFERENCE: print('Shapes:', train.shape, target.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred, return_components=False) -> float:\n",
    "    \"\"\"Amex metric for ndarrays\"\"\"\n",
    "    def top_four_percent_captured(df) -> float:\n",
    "        \"\"\"Corresponds to the recall for a threshold of 4 %\"\"\"\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(df) -> float:\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(df) -> float:\n",
    "        \"\"\"Corresponds to 2 * AUC - 1\"\"\"\n",
    "        df2 = pd.DataFrame({'target': df.target, 'prediction': df.target})\n",
    "        df2.sort_values('prediction', ascending=False, inplace=True)\n",
    "        return weighted_gini(df) / weighted_gini(df2)\n",
    "\n",
    "    df = pd.DataFrame({'target': y_true.ravel(), 'prediction': y_pred.ravel()})\n",
    "    df.sort_values('prediction', ascending=False, inplace=True)\n",
    "    g = normalized_weighted_gini(df)\n",
    "    d = top_four_percent_captured(df)\n",
    "\n",
    "    if return_components: return g, d, 0.5 * (g + d)\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "def lgb_amex_metric(y_true, y_pred):\n",
    "    \"\"\"The competition metric with lightgbm's calling convention\"\"\"\n",
    "    return ('amex',\n",
    "            amex_metric(y_true, y_pred),\n",
    "            True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 features**\n",
      "[100]\ttraining's binary_logloss: 0.251047\ttraining's amex: 0.768483\tvalid_1's binary_logloss: 0.256684\tvalid_1's amex: 0.759042\n",
      "[200]\ttraining's binary_logloss: 0.222507\ttraining's amex: 0.788701\tvalid_1's binary_logloss: 0.232554\tvalid_1's amex: 0.773801\n",
      "[300]\ttraining's binary_logloss: 0.211888\ttraining's amex: 0.801389\tvalid_1's binary_logloss: 0.226414\tvalid_1's amex: 0.780336\n",
      "[400]\ttraining's binary_logloss: 0.205147\ttraining's amex: 0.81059\tvalid_1's binary_logloss: 0.22379\tvalid_1's amex: 0.784276\n",
      "[500]\ttraining's binary_logloss: 0.199999\ttraining's amex: 0.818163\tvalid_1's binary_logloss: 0.222576\tvalid_1's amex: 0.786652\n",
      "[600]\ttraining's binary_logloss: 0.195581\ttraining's amex: 0.824723\tvalid_1's binary_logloss: 0.221833\tvalid_1's amex: 0.787852\n",
      "[700]\ttraining's binary_logloss: 0.191347\ttraining's amex: 0.831122\tvalid_1's binary_logloss: 0.221346\tvalid_1's amex: 0.78902\n",
      "[800]\ttraining's binary_logloss: 0.187358\ttraining's amex: 0.83732\tvalid_1's binary_logloss: 0.221083\tvalid_1's amex: 0.7895\n",
      "[900]\ttraining's binary_logloss: 0.183525\ttraining's amex: 0.84342\tvalid_1's binary_logloss: 0.220857\tvalid_1's amex: 0.790723\n",
      "[1000]\ttraining's binary_logloss: 0.179857\ttraining's amex: 0.849589\tvalid_1's binary_logloss: 0.220788\tvalid_1's amex: 0.791151\n",
      "[1100]\ttraining's binary_logloss: 0.17646\ttraining's amex: 0.855524\tvalid_1's binary_logloss: 0.22074\tvalid_1's amex: 0.79088\n",
      "[1200]\ttraining's binary_logloss: 0.173205\ttraining's amex: 0.860904\tvalid_1's binary_logloss: 0.220725\tvalid_1's amex: 0.790787\n",
      "[1300]\ttraining's binary_logloss: 0.170006\ttraining's amex: 0.86648\tvalid_1's binary_logloss: 0.220747\tvalid_1's amex: 0.791001\n",
      "\u001b[32m\u001b[1mFold 0 | 15:21 |  1332 trees |                Score = 0.79151\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.251101\ttraining's amex: 0.769823\tvalid_1's binary_logloss: 0.255399\tvalid_1's amex: 0.756191\n",
      "[200]\ttraining's binary_logloss: 0.222806\ttraining's amex: 0.78884\tvalid_1's binary_logloss: 0.231181\tvalid_1's amex: 0.771983\n",
      "[300]\ttraining's binary_logloss: 0.212046\ttraining's amex: 0.801581\tvalid_1's binary_logloss: 0.225054\tvalid_1's amex: 0.78036\n",
      "[400]\ttraining's binary_logloss: 0.205253\ttraining's amex: 0.810707\tvalid_1's binary_logloss: 0.22259\tvalid_1's amex: 0.783757\n",
      "[500]\ttraining's binary_logloss: 0.20019\ttraining's amex: 0.8179\tvalid_1's binary_logloss: 0.221505\tvalid_1's amex: 0.785797\n",
      "[600]\ttraining's binary_logloss: 0.195584\ttraining's amex: 0.825284\tvalid_1's binary_logloss: 0.220961\tvalid_1's amex: 0.786774\n",
      "[700]\ttraining's binary_logloss: 0.191426\ttraining's amex: 0.830902\tvalid_1's binary_logloss: 0.220577\tvalid_1's amex: 0.787718\n",
      "[800]\ttraining's binary_logloss: 0.187489\ttraining's amex: 0.837147\tvalid_1's binary_logloss: 0.220391\tvalid_1's amex: 0.78758\n",
      "[900]\ttraining's binary_logloss: 0.183726\ttraining's amex: 0.843246\tvalid_1's binary_logloss: 0.220242\tvalid_1's amex: 0.788144\n",
      "[1000]\ttraining's binary_logloss: 0.179963\ttraining's amex: 0.849368\tvalid_1's binary_logloss: 0.220167\tvalid_1's amex: 0.787675\n",
      "[1100]\ttraining's binary_logloss: 0.176482\ttraining's amex: 0.855299\tvalid_1's binary_logloss: 0.220128\tvalid_1's amex: 0.788115\n",
      "[1200]\ttraining's binary_logloss: 0.173229\ttraining's amex: 0.860609\tvalid_1's binary_logloss: 0.220139\tvalid_1's amex: 0.788123\n",
      "[1300]\ttraining's binary_logloss: 0.169948\ttraining's amex: 0.866303\tvalid_1's binary_logloss: 0.22016\tvalid_1's amex: 0.788082\n",
      "\u001b[32m\u001b[1mFold 1 | 15:19 |  1332 trees |                Score = 0.78830\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.251435\ttraining's amex: 0.767597\tvalid_1's binary_logloss: 0.254818\tvalid_1's amex: 0.762164\n",
      "[200]\ttraining's binary_logloss: 0.223099\ttraining's amex: 0.788574\tvalid_1's binary_logloss: 0.229953\tvalid_1's amex: 0.775511\n",
      "[300]\ttraining's binary_logloss: 0.212506\ttraining's amex: 0.801763\tvalid_1's binary_logloss: 0.223675\tvalid_1's amex: 0.783026\n",
      "[400]\ttraining's binary_logloss: 0.205741\ttraining's amex: 0.811322\tvalid_1's binary_logloss: 0.22124\tvalid_1's amex: 0.784437\n",
      "[500]\ttraining's binary_logloss: 0.200379\ttraining's amex: 0.819305\tvalid_1's binary_logloss: 0.220036\tvalid_1's amex: 0.786414\n",
      "[600]\ttraining's binary_logloss: 0.195717\ttraining's amex: 0.825457\tvalid_1's binary_logloss: 0.219363\tvalid_1's amex: 0.786986\n",
      "[700]\ttraining's binary_logloss: 0.191594\ttraining's amex: 0.831687\tvalid_1's binary_logloss: 0.218948\tvalid_1's amex: 0.787069\n",
      "[800]\ttraining's binary_logloss: 0.187639\ttraining's amex: 0.838145\tvalid_1's binary_logloss: 0.218711\tvalid_1's amex: 0.787996\n",
      "[900]\ttraining's binary_logloss: 0.183881\ttraining's amex: 0.843851\tvalid_1's binary_logloss: 0.218528\tvalid_1's amex: 0.788229\n",
      "[1000]\ttraining's binary_logloss: 0.180439\ttraining's amex: 0.849619\tvalid_1's binary_logloss: 0.218438\tvalid_1's amex: 0.788704\n",
      "[1100]\ttraining's binary_logloss: 0.177178\ttraining's amex: 0.855515\tvalid_1's binary_logloss: 0.218376\tvalid_1's amex: 0.789234\n",
      "[1200]\ttraining's binary_logloss: 0.173867\ttraining's amex: 0.861359\tvalid_1's binary_logloss: 0.218375\tvalid_1's amex: 0.789027\n",
      "[1300]\ttraining's binary_logloss: 0.170833\ttraining's amex: 0.866684\tvalid_1's binary_logloss: 0.218309\tvalid_1's amex: 0.789075\n",
      "\u001b[32m\u001b[1mFold 2 | 15:20 |  1332 trees |                Score = 0.78882\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.251795\ttraining's amex: 0.767617\tvalid_1's binary_logloss: 0.25358\tvalid_1's amex: 0.762232\n",
      "[200]\ttraining's binary_logloss: 0.223301\ttraining's amex: 0.787446\tvalid_1's binary_logloss: 0.228553\tvalid_1's amex: 0.77754\n",
      "[300]\ttraining's binary_logloss: 0.212548\ttraining's amex: 0.801645\tvalid_1's binary_logloss: 0.222391\tvalid_1's amex: 0.784826\n",
      "[400]\ttraining's binary_logloss: 0.205909\ttraining's amex: 0.81015\tvalid_1's binary_logloss: 0.219908\tvalid_1's amex: 0.788828\n",
      "[500]\ttraining's binary_logloss: 0.200896\ttraining's amex: 0.817877\tvalid_1's binary_logloss: 0.21883\tvalid_1's amex: 0.79009\n",
      "[600]\ttraining's binary_logloss: 0.196403\ttraining's amex: 0.824174\tvalid_1's binary_logloss: 0.218205\tvalid_1's amex: 0.791019\n",
      "[700]\ttraining's binary_logloss: 0.192181\ttraining's amex: 0.830443\tvalid_1's binary_logloss: 0.217824\tvalid_1's amex: 0.790689\n",
      "[800]\ttraining's binary_logloss: 0.188194\ttraining's amex: 0.836881\tvalid_1's binary_logloss: 0.217551\tvalid_1's amex: 0.792043\n",
      "[900]\ttraining's binary_logloss: 0.184573\ttraining's amex: 0.842775\tvalid_1's binary_logloss: 0.217392\tvalid_1's amex: 0.791695\n",
      "[1000]\ttraining's binary_logloss: 0.180807\ttraining's amex: 0.848915\tvalid_1's binary_logloss: 0.217254\tvalid_1's amex: 0.791529\n",
      "[1100]\ttraining's binary_logloss: 0.177386\ttraining's amex: 0.854952\tvalid_1's binary_logloss: 0.217227\tvalid_1's amex: 0.791855\n",
      "[1200]\ttraining's binary_logloss: 0.174085\ttraining's amex: 0.860724\tvalid_1's binary_logloss: 0.21723\tvalid_1's amex: 0.792065\n",
      "[1300]\ttraining's binary_logloss: 0.170905\ttraining's amex: 0.866226\tvalid_1's binary_logloss: 0.21722\tvalid_1's amex: 0.792154\n",
      "\u001b[32m\u001b[1mFold 3 | 15:12 |  1332 trees |                Score = 0.79142\u001b[0m\n",
      "[100]\ttraining's binary_logloss: 0.251798\ttraining's amex: 0.767364\tvalid_1's binary_logloss: 0.253668\tvalid_1's amex: 0.762144\n",
      "[200]\ttraining's binary_logloss: 0.223451\ttraining's amex: 0.787434\tvalid_1's binary_logloss: 0.229048\tvalid_1's amex: 0.776992\n",
      "[300]\ttraining's binary_logloss: 0.212834\ttraining's amex: 0.800726\tvalid_1's binary_logloss: 0.222867\tvalid_1's amex: 0.784676\n",
      "[400]\ttraining's binary_logloss: 0.206119\ttraining's amex: 0.809642\tvalid_1's binary_logloss: 0.220417\tvalid_1's amex: 0.788358\n",
      "[500]\ttraining's binary_logloss: 0.200922\ttraining's amex: 0.817436\tvalid_1's binary_logloss: 0.219321\tvalid_1's amex: 0.789478\n",
      "[600]\ttraining's binary_logloss: 0.196289\ttraining's amex: 0.823944\tvalid_1's binary_logloss: 0.218722\tvalid_1's amex: 0.790465\n",
      "[700]\ttraining's binary_logloss: 0.192019\ttraining's amex: 0.830655\tvalid_1's binary_logloss: 0.218394\tvalid_1's amex: 0.790078\n",
      "[800]\ttraining's binary_logloss: 0.188038\ttraining's amex: 0.837152\tvalid_1's binary_logloss: 0.218177\tvalid_1's amex: 0.790596\n",
      "[900]\ttraining's binary_logloss: 0.184139\ttraining's amex: 0.843184\tvalid_1's binary_logloss: 0.218091\tvalid_1's amex: 0.790965\n",
      "[1000]\ttraining's binary_logloss: 0.180514\ttraining's amex: 0.849398\tvalid_1's binary_logloss: 0.218078\tvalid_1's amex: 0.791034\n",
      "[1100]\ttraining's binary_logloss: 0.176929\ttraining's amex: 0.855279\tvalid_1's binary_logloss: 0.218071\tvalid_1's amex: 0.790682\n",
      "[1200]\ttraining's binary_logloss: 0.17365\ttraining's amex: 0.860755\tvalid_1's binary_logloss: 0.218071\tvalid_1's amex: 0.79096\n",
      "[1300]\ttraining's binary_logloss: 0.170474\ttraining's amex: 0.866333\tvalid_1's binary_logloss: 0.218143\tvalid_1's amex: 0.790643\n",
      "\u001b[32m\u001b[1mFold 4 | 15:07 |  1332 trees |                Score = 0.79058\u001b[0m\n",
      "\u001b[32m\u001b[1mOOF Score:                       0.79013\u001b[0m\n",
      "CPU times: user 14h 49min 32s, sys: 15min 13s, total: 15h 4min 45s\n",
      "Wall time: 1h 17min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ONLY_FIRST_FOLD = False\n",
    "\n",
    "features = [f for f in train.columns if f != 'customer_ID' and f != 'target']\n",
    "\n",
    "def my_booster():\n",
    "    params = {'learning_rate': 0.02410649590217162, 'n_estimators': 1332, 'max_depth': 16, 'min_child_samples': 3551, 'max_bin': 582, 'num_leaves': 929, 'random_state':42}\n",
    "    \n",
    "    return LGBMClassifier(**params)\n",
    "\n",
    "print(f'{len(features)} features**')\n",
    "\n",
    "score_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(train, target)):\n",
    "    start_time = datetime.datetime.now()\n",
    "    X_tr = train.iloc[idx_tr][features]\n",
    "    X_va = train.iloc[idx_va][features]\n",
    "    y_tr = target.iloc[idx_tr]\n",
    "    y_va = target.iloc[idx_va]\n",
    "\n",
    "    model = my_booster()\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', category=UserWarning)\n",
    "        model.fit(X_tr, y_tr,\n",
    "                eval_set=[(X_tr, y_tr), (X_va, y_va)],\n",
    "                eval_metric=[lgb_amex_metric],\n",
    "                callbacks=[log_evaluation(100)]) ##\n",
    "    y_va_pred = model.predict_proba(X_va)[:, 1]\n",
    "    score = amex_metric(y_va.values, y_va_pred)\n",
    "    n_trees = model.best_iteration_\n",
    "\n",
    "    if n_trees is None:\n",
    "        n_trees = model.n_estimators\n",
    "        \n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT}Fold {fold} | {str(datetime.datetime.now() - start_time)[-12:-7]} |\"\n",
    "          f\" {n_trees:5} trees |\"\n",
    "          f\"                Score = {score:.5f}{Style.RESET_ALL}\")\n",
    "    score_list.append(score)\n",
    "    \n",
    "    if INFERENCE:\n",
    "        y_pred_list.append(model.predict_proba(test[features])[:,1])\n",
    "        \n",
    "    if ONLY_FIRST_FOLD: break # we only want the first fold\n",
    "    \n",
    "print(f\"{Fore.GREEN}{Style.BRIGHT}OOF Score:                       {np.mean(score_list):.5f}{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "  param = {\n",
    "      \"random_state\":42,\n",
    "      'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 0.05),\n",
    "      \"n_estimators\":trial.suggest_int(\"n_estimators\", 500, 2500),\n",
    "      \"max_depth\":trial.suggest_int(\"max_depth\", 6, 16),\n",
    "      \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 2000, 4000),\n",
    "      \"max_bin\": trial.suggest_int(\"max_bin\", 300, 600),\n",
    "      'num_leaves': trial.suggest_int(\"num_leaves\", 127, 1023),\n",
    "      'boosting_type': trial.suggest_categorical('boosting_type', ['dart', 'gbdt']),\n",
    "  }\n",
    "\n",
    "\n",
    "  lgbm = LGBMClassifier(**param)\n",
    "  lgbm.fit(X_tr, y_tr,\n",
    "                eval_set=[(X_tr, y_tr), (X_va, y_va)],\n",
    "                eval_metric=[lgb_amex_metric],\n",
    "                callbacks=[log_evaluation(100)])\n",
    "  preds = lgbm.predict(X_va)\n",
    "  score = amex_metric(y_va, preds)\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 16:29:22,927]\u001b[0m A new study created in memory with name: lgbm_parameter_opt\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.269397\ttraining's amex: 0.764\tvalid_1's binary_logloss: 0.272954\tvalid_1's amex: 0.751217\n",
      "[200]\ttraining's binary_logloss: 0.230861\ttraining's amex: 0.782072\tvalid_1's binary_logloss: 0.237578\tvalid_1's amex: 0.765287\n",
      "[300]\ttraining's binary_logloss: 0.21856\ttraining's amex: 0.794064\tvalid_1's binary_logloss: 0.228748\tvalid_1's amex: 0.774286\n",
      "[400]\ttraining's binary_logloss: 0.211505\ttraining's amex: 0.802691\tvalid_1's binary_logloss: 0.224942\tvalid_1's amex: 0.780132\n",
      "[500]\ttraining's binary_logloss: 0.206373\ttraining's amex: 0.809756\tvalid_1's binary_logloss: 0.222914\tvalid_1's amex: 0.783227\n",
      "[600]\ttraining's binary_logloss: 0.202437\ttraining's amex: 0.815101\tvalid_1's binary_logloss: 0.221927\tvalid_1's amex: 0.784067\n",
      "[700]\ttraining's binary_logloss: 0.198902\ttraining's amex: 0.820192\tvalid_1's binary_logloss: 0.221338\tvalid_1's amex: 0.784556\n",
      "[800]\ttraining's binary_logloss: 0.19571\ttraining's amex: 0.825192\tvalid_1's binary_logloss: 0.22096\tvalid_1's amex: 0.785391\n",
      "[900]\ttraining's binary_logloss: 0.192537\ttraining's amex: 0.830363\tvalid_1's binary_logloss: 0.220692\tvalid_1's amex: 0.786143\n",
      "[1000]\ttraining's binary_logloss: 0.189719\ttraining's amex: 0.834979\tvalid_1's binary_logloss: 0.220525\tvalid_1's amex: 0.786627\n",
      "[1100]\ttraining's binary_logloss: 0.186994\ttraining's amex: 0.839083\tvalid_1's binary_logloss: 0.220381\tvalid_1's amex: 0.786621\n",
      "[1200]\ttraining's binary_logloss: 0.184385\ttraining's amex: 0.843354\tvalid_1's binary_logloss: 0.220303\tvalid_1's amex: 0.787117\n",
      "[1300]\ttraining's binary_logloss: 0.181796\ttraining's amex: 0.847557\tvalid_1's binary_logloss: 0.220239\tvalid_1's amex: 0.787522\n",
      "[1400]\ttraining's binary_logloss: 0.179264\ttraining's amex: 0.851729\tvalid_1's binary_logloss: 0.220144\tvalid_1's amex: 0.787136\n",
      "[1500]\ttraining's binary_logloss: 0.176806\ttraining's amex: 0.855434\tvalid_1's binary_logloss: 0.220121\tvalid_1's amex: 0.787129\n",
      "[1600]\ttraining's binary_logloss: 0.174562\ttraining's amex: 0.859393\tvalid_1's binary_logloss: 0.220102\tvalid_1's amex: 0.78678\n",
      "[1700]\ttraining's binary_logloss: 0.172296\ttraining's amex: 0.863447\tvalid_1's binary_logloss: 0.220118\tvalid_1's amex: 0.787032\n",
      "[1800]\ttraining's binary_logloss: 0.170008\ttraining's amex: 0.867358\tvalid_1's binary_logloss: 0.220099\tvalid_1's amex: 0.787236\n",
      "[1900]\ttraining's binary_logloss: 0.167834\ttraining's amex: 0.871107\tvalid_1's binary_logloss: 0.220095\tvalid_1's amex: 0.786927\n",
      "[2000]\ttraining's binary_logloss: 0.165664\ttraining's amex: 0.874565\tvalid_1's binary_logloss: 0.220083\tvalid_1's amex: 0.787339\n",
      "[2100]\ttraining's binary_logloss: 0.163536\ttraining's amex: 0.878573\tvalid_1's binary_logloss: 0.220111\tvalid_1's amex: 0.787839\n",
      "[2200]\ttraining's binary_logloss: 0.161343\ttraining's amex: 0.882412\tvalid_1's binary_logloss: 0.220138\tvalid_1's amex: 0.787059\n",
      "[2300]\ttraining's binary_logloss: 0.159409\ttraining's amex: 0.886127\tvalid_1's binary_logloss: 0.220136\tvalid_1's amex: 0.787066\n",
      "[2400]\ttraining's binary_logloss: 0.157547\ttraining's amex: 0.889367\tvalid_1's binary_logloss: 0.220221\tvalid_1's amex: 0.787229\n",
      "[2500]\ttraining's binary_logloss: 0.155552\ttraining's amex: 0.893082\tvalid_1's binary_logloss: 0.220253\tvalid_1's amex: 0.787417\n",
      "[2600]\ttraining's binary_logloss: 0.153476\ttraining's amex: 0.896849\tvalid_1's binary_logloss: 0.220336\tvalid_1's amex: 0.787207\n",
      "[2700]\ttraining's binary_logloss: 0.151714\ttraining's amex: 0.900209\tvalid_1's binary_logloss: 0.220395\tvalid_1's amex: 0.78677\n",
      "[2800]\ttraining's binary_logloss: 0.149881\ttraining's amex: 0.903529\tvalid_1's binary_logloss: 0.220468\tvalid_1's amex: 0.786757\n",
      "[2900]\ttraining's binary_logloss: 0.148163\ttraining's amex: 0.906744\tvalid_1's binary_logloss: 0.220525\tvalid_1's amex: 0.786874\n",
      "[3000]\ttraining's binary_logloss: 0.14632\ttraining's amex: 0.909968\tvalid_1's binary_logloss: 0.220587\tvalid_1's amex: 0.786567\n",
      "[3100]\ttraining's binary_logloss: 0.144641\ttraining's amex: 0.912819\tvalid_1's binary_logloss: 0.22064\tvalid_1's amex: 0.786434\n",
      "[3200]\ttraining's binary_logloss: 0.142724\ttraining's amex: 0.916403\tvalid_1's binary_logloss: 0.220699\tvalid_1's amex: 0.786677\n",
      "[3300]\ttraining's binary_logloss: 0.140932\ttraining's amex: 0.91937\tvalid_1's binary_logloss: 0.220773\tvalid_1's amex: 0.786959\n",
      "[3400]\ttraining's binary_logloss: 0.139039\ttraining's amex: 0.922715\tvalid_1's binary_logloss: 0.220836\tvalid_1's amex: 0.786801\n",
      "[3500]\ttraining's binary_logloss: 0.137247\ttraining's amex: 0.926068\tvalid_1's binary_logloss: 0.220937\tvalid_1's amex: 0.787414\n",
      "[3600]\ttraining's binary_logloss: 0.135581\ttraining's amex: 0.928851\tvalid_1's binary_logloss: 0.221011\tvalid_1's amex: 0.787023\n",
      "[3700]\ttraining's binary_logloss: 0.13415\ttraining's amex: 0.931437\tvalid_1's binary_logloss: 0.221088\tvalid_1's amex: 0.787494\n",
      "[3800]\ttraining's binary_logloss: 0.132693\ttraining's amex: 0.933992\tvalid_1's binary_logloss: 0.22117\tvalid_1's amex: 0.786955\n",
      "[3900]\ttraining's binary_logloss: 0.131269\ttraining's amex: 0.936518\tvalid_1's binary_logloss: 0.221237\tvalid_1's amex: 0.787219\n",
      "[4000]\ttraining's binary_logloss: 0.12973\ttraining's amex: 0.938988\tvalid_1's binary_logloss: 0.221353\tvalid_1's amex: 0.786944\n",
      "[4100]\ttraining's binary_logloss: 0.128281\ttraining's amex: 0.941289\tvalid_1's binary_logloss: 0.221441\tvalid_1's amex: 0.787267\n",
      "[4200]\ttraining's binary_logloss: 0.126975\ttraining's amex: 0.943628\tvalid_1's binary_logloss: 0.221516\tvalid_1's amex: 0.78715\n",
      "[4300]\ttraining's binary_logloss: 0.125543\ttraining's amex: 0.946124\tvalid_1's binary_logloss: 0.22161\tvalid_1's amex: 0.787408\n",
      "[4400]\ttraining's binary_logloss: 0.1242\ttraining's amex: 0.948407\tvalid_1's binary_logloss: 0.221722\tvalid_1's amex: 0.78684\n",
      "[4500]\ttraining's binary_logloss: 0.122754\ttraining's amex: 0.950727\tvalid_1's binary_logloss: 0.221812\tvalid_1's amex: 0.787099\n",
      "[4600]\ttraining's binary_logloss: 0.121357\ttraining's amex: 0.952733\tvalid_1's binary_logloss: 0.22192\tvalid_1's amex: 0.786936\n",
      "[4700]\ttraining's binary_logloss: 0.120052\ttraining's amex: 0.95478\tvalid_1's binary_logloss: 0.222046\tvalid_1's amex: 0.787251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 17:21:22,422]\u001b[0m Trial 0 finished with value: 0.5783413441846182 and parameters: {'learning_rate': 0.018272261776066247, 'n_estimators': 4779, 'max_depth': 14, 'min_child_samples': 3197, 'max_bin': 346, 'num_leaves': 266}. Best is trial 0 with value: 0.5783413441846182.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.31712\ttraining's amex: 0.751937\tvalid_1's binary_logloss: 0.319145\tvalid_1's amex: 0.741818\n",
      "[200]\ttraining's binary_logloss: 0.256949\ttraining's amex: 0.766951\tvalid_1's binary_logloss: 0.260921\tvalid_1's amex: 0.755016\n",
      "[300]\ttraining's binary_logloss: 0.235547\ttraining's amex: 0.778047\tvalid_1's binary_logloss: 0.241317\tvalid_1's amex: 0.763133\n",
      "[400]\ttraining's binary_logloss: 0.225717\ttraining's amex: 0.785978\tvalid_1's binary_logloss: 0.233265\tvalid_1's amex: 0.769051\n",
      "[500]\ttraining's binary_logloss: 0.21985\ttraining's amex: 0.792448\tvalid_1's binary_logloss: 0.229171\tvalid_1's amex: 0.772849\n",
      "[600]\ttraining's binary_logloss: 0.215621\ttraining's amex: 0.797377\tvalid_1's binary_logloss: 0.226607\tvalid_1's amex: 0.776909\n",
      "[700]\ttraining's binary_logloss: 0.21232\ttraining's amex: 0.801559\tvalid_1's binary_logloss: 0.224857\tvalid_1's amex: 0.780336\n",
      "[800]\ttraining's binary_logloss: 0.20947\ttraining's amex: 0.805113\tvalid_1's binary_logloss: 0.223579\tvalid_1's amex: 0.782133\n",
      "[900]\ttraining's binary_logloss: 0.207128\ttraining's amex: 0.808333\tvalid_1's binary_logloss: 0.222746\tvalid_1's amex: 0.783081\n",
      "[1000]\ttraining's binary_logloss: 0.205148\ttraining's amex: 0.811105\tvalid_1's binary_logloss: 0.22215\tvalid_1's amex: 0.784076\n",
      "[1100]\ttraining's binary_logloss: 0.203298\ttraining's amex: 0.814059\tvalid_1's binary_logloss: 0.221747\tvalid_1's amex: 0.784414\n",
      "[1200]\ttraining's binary_logloss: 0.201503\ttraining's amex: 0.816606\tvalid_1's binary_logloss: 0.221389\tvalid_1's amex: 0.785029\n",
      "[1300]\ttraining's binary_logloss: 0.199769\ttraining's amex: 0.819329\tvalid_1's binary_logloss: 0.221123\tvalid_1's amex: 0.785339\n",
      "[1400]\ttraining's binary_logloss: 0.198226\ttraining's amex: 0.821833\tvalid_1's binary_logloss: 0.220909\tvalid_1's amex: 0.784704\n",
      "[1500]\ttraining's binary_logloss: 0.196791\ttraining's amex: 0.823925\tvalid_1's binary_logloss: 0.220739\tvalid_1's amex: 0.785271\n",
      "[1600]\ttraining's binary_logloss: 0.195426\ttraining's amex: 0.825852\tvalid_1's binary_logloss: 0.220602\tvalid_1's amex: 0.785616\n",
      "[1700]\ttraining's binary_logloss: 0.19413\ttraining's amex: 0.827941\tvalid_1's binary_logloss: 0.220483\tvalid_1's amex: 0.786249\n",
      "[1800]\ttraining's binary_logloss: 0.192712\ttraining's amex: 0.83029\tvalid_1's binary_logloss: 0.220371\tvalid_1's amex: 0.786374\n",
      "[1900]\ttraining's binary_logloss: 0.191309\ttraining's amex: 0.832481\tvalid_1's binary_logloss: 0.220293\tvalid_1's amex: 0.786383\n",
      "[2000]\ttraining's binary_logloss: 0.189938\ttraining's amex: 0.834445\tvalid_1's binary_logloss: 0.22022\tvalid_1's amex: 0.786536\n",
      "[2100]\ttraining's binary_logloss: 0.188663\ttraining's amex: 0.836752\tvalid_1's binary_logloss: 0.220145\tvalid_1's amex: 0.787007\n",
      "[2200]\ttraining's binary_logloss: 0.187418\ttraining's amex: 0.838709\tvalid_1's binary_logloss: 0.22009\tvalid_1's amex: 0.786965\n",
      "[2300]\ttraining's binary_logloss: 0.186202\ttraining's amex: 0.840672\tvalid_1's binary_logloss: 0.220034\tvalid_1's amex: 0.786776\n",
      "[2400]\ttraining's binary_logloss: 0.184945\ttraining's amex: 0.842735\tvalid_1's binary_logloss: 0.219962\tvalid_1's amex: 0.786761\n",
      "[2500]\ttraining's binary_logloss: 0.183664\ttraining's amex: 0.844833\tvalid_1's binary_logloss: 0.219943\tvalid_1's amex: 0.78721\n",
      "[2600]\ttraining's binary_logloss: 0.182336\ttraining's amex: 0.846931\tvalid_1's binary_logloss: 0.219923\tvalid_1's amex: 0.787428\n",
      "[2700]\ttraining's binary_logloss: 0.180974\ttraining's amex: 0.849095\tvalid_1's binary_logloss: 0.219914\tvalid_1's amex: 0.787981\n",
      "[2800]\ttraining's binary_logloss: 0.179651\ttraining's amex: 0.85133\tvalid_1's binary_logloss: 0.219905\tvalid_1's amex: 0.787669\n",
      "[2900]\ttraining's binary_logloss: 0.178453\ttraining's amex: 0.853453\tvalid_1's binary_logloss: 0.219904\tvalid_1's amex: 0.787315\n",
      "[3000]\ttraining's binary_logloss: 0.177375\ttraining's amex: 0.855318\tvalid_1's binary_logloss: 0.219911\tvalid_1's amex: 0.787671\n",
      "[3100]\ttraining's binary_logloss: 0.176249\ttraining's amex: 0.857104\tvalid_1's binary_logloss: 0.219932\tvalid_1's amex: 0.787266\n",
      "[3200]\ttraining's binary_logloss: 0.175317\ttraining's amex: 0.85859\tvalid_1's binary_logloss: 0.219915\tvalid_1's amex: 0.787697\n",
      "[3300]\ttraining's binary_logloss: 0.174231\ttraining's amex: 0.860569\tvalid_1's binary_logloss: 0.219919\tvalid_1's amex: 0.787003\n",
      "[3400]\ttraining's binary_logloss: 0.173161\ttraining's amex: 0.862395\tvalid_1's binary_logloss: 0.219901\tvalid_1's amex: 0.787179\n",
      "[3500]\ttraining's binary_logloss: 0.172059\ttraining's amex: 0.864201\tvalid_1's binary_logloss: 0.219896\tvalid_1's amex: 0.78729\n",
      "[3600]\ttraining's binary_logloss: 0.170867\ttraining's amex: 0.866054\tvalid_1's binary_logloss: 0.219892\tvalid_1's amex: 0.786893\n",
      "[3700]\ttraining's binary_logloss: 0.169812\ttraining's amex: 0.867925\tvalid_1's binary_logloss: 0.21993\tvalid_1's amex: 0.786966\n",
      "[3800]\ttraining's binary_logloss: 0.168679\ttraining's amex: 0.870159\tvalid_1's binary_logloss: 0.219916\tvalid_1's amex: 0.786932\n",
      "[3900]\ttraining's binary_logloss: 0.167598\ttraining's amex: 0.871918\tvalid_1's binary_logloss: 0.219911\tvalid_1's amex: 0.786894\n",
      "[4000]\ttraining's binary_logloss: 0.166512\ttraining's amex: 0.873754\tvalid_1's binary_logloss: 0.219909\tvalid_1's amex: 0.787299\n",
      "[4100]\ttraining's binary_logloss: 0.165553\ttraining's amex: 0.875475\tvalid_1's binary_logloss: 0.219921\tvalid_1's amex: 0.786941\n",
      "[4200]\ttraining's binary_logloss: 0.16462\ttraining's amex: 0.877086\tvalid_1's binary_logloss: 0.219942\tvalid_1's amex: 0.787209\n",
      "[4300]\ttraining's binary_logloss: 0.163675\ttraining's amex: 0.87869\tvalid_1's binary_logloss: 0.219942\tvalid_1's amex: 0.787318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 18:09:06,788]\u001b[0m Trial 1 finished with value: 0.5800837189391985 and parameters: {'learning_rate': 0.010979908036596667, 'n_estimators': 4398, 'max_depth': 12, 'min_child_samples': 3416, 'max_bin': 306, 'num_leaves': 997}. Best is trial 0 with value: 0.5783413441846182.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.229561\ttraining's amex: 0.783128\tvalid_1's binary_logloss: 0.236774\tvalid_1's amex: 0.765692\n",
      "[200]\ttraining's binary_logloss: 0.214002\ttraining's amex: 0.799823\tvalid_1's binary_logloss: 0.226031\tvalid_1's amex: 0.777217\n",
      "[300]\ttraining's binary_logloss: 0.207423\ttraining's amex: 0.809291\tvalid_1's binary_logloss: 0.222946\tvalid_1's amex: 0.783264\n",
      "[400]\ttraining's binary_logloss: 0.203169\ttraining's amex: 0.815504\tvalid_1's binary_logloss: 0.221883\tvalid_1's amex: 0.784621\n",
      "[500]\ttraining's binary_logloss: 0.199296\ttraining's amex: 0.821759\tvalid_1's binary_logloss: 0.221377\tvalid_1's amex: 0.784937\n",
      "[600]\ttraining's binary_logloss: 0.195784\ttraining's amex: 0.827616\tvalid_1's binary_logloss: 0.221034\tvalid_1's amex: 0.785514\n",
      "[700]\ttraining's binary_logloss: 0.192473\ttraining's amex: 0.833031\tvalid_1's binary_logloss: 0.220808\tvalid_1's amex: 0.785415\n",
      "[800]\ttraining's binary_logloss: 0.189483\ttraining's amex: 0.837642\tvalid_1's binary_logloss: 0.220695\tvalid_1's amex: 0.786554\n",
      "[900]\ttraining's binary_logloss: 0.186446\ttraining's amex: 0.842686\tvalid_1's binary_logloss: 0.220642\tvalid_1's amex: 0.785886\n",
      "[1000]\ttraining's binary_logloss: 0.18334\ttraining's amex: 0.847866\tvalid_1's binary_logloss: 0.220579\tvalid_1's amex: 0.785936\n",
      "[1100]\ttraining's binary_logloss: 0.180396\ttraining's amex: 0.85249\tvalid_1's binary_logloss: 0.220546\tvalid_1's amex: 0.786207\n",
      "[1200]\ttraining's binary_logloss: 0.17764\ttraining's amex: 0.856914\tvalid_1's binary_logloss: 0.220642\tvalid_1's amex: 0.785823\n",
      "[1300]\ttraining's binary_logloss: 0.174897\ttraining's amex: 0.861736\tvalid_1's binary_logloss: 0.220673\tvalid_1's amex: 0.785358\n",
      "[1400]\ttraining's binary_logloss: 0.172276\ttraining's amex: 0.866501\tvalid_1's binary_logloss: 0.220696\tvalid_1's amex: 0.784916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 18:23:54,701]\u001b[0m Trial 2 finished with value: 0.5798886834370937 and parameters: {'learning_rate': 0.03818145165896871, 'n_estimators': 1455, 'max_depth': 8, 'min_child_samples': 2366, 'max_bin': 391, 'num_leaves': 597}. Best is trial 0 with value: 0.5783413441846182.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.259485\ttraining's amex: 0.770816\tvalid_1's binary_logloss: 0.264277\tvalid_1's amex: 0.754524\n",
      "[200]\ttraining's binary_logloss: 0.224592\ttraining's amex: 0.788904\tvalid_1's binary_logloss: 0.233657\tvalid_1's amex: 0.769305\n",
      "[300]\ttraining's binary_logloss: 0.213384\ttraining's amex: 0.800955\tvalid_1's binary_logloss: 0.226594\tvalid_1's amex: 0.777425\n",
      "[400]\ttraining's binary_logloss: 0.206732\ttraining's amex: 0.809817\tvalid_1's binary_logloss: 0.223599\tvalid_1's amex: 0.782178\n",
      "[500]\ttraining's binary_logloss: 0.201767\ttraining's amex: 0.817517\tvalid_1's binary_logloss: 0.222156\tvalid_1's amex: 0.785072\n",
      "[600]\ttraining's binary_logloss: 0.197748\ttraining's amex: 0.823162\tvalid_1's binary_logloss: 0.221463\tvalid_1's amex: 0.786016\n",
      "[700]\ttraining's binary_logloss: 0.194159\ttraining's amex: 0.828246\tvalid_1's binary_logloss: 0.221021\tvalid_1's amex: 0.786434\n",
      "[800]\ttraining's binary_logloss: 0.190866\ttraining's amex: 0.833712\tvalid_1's binary_logloss: 0.220761\tvalid_1's amex: 0.786404\n",
      "[900]\ttraining's binary_logloss: 0.187661\ttraining's amex: 0.838744\tvalid_1's binary_logloss: 0.220639\tvalid_1's amex: 0.786451\n",
      "[1000]\ttraining's binary_logloss: 0.184475\ttraining's amex: 0.843841\tvalid_1's binary_logloss: 0.220538\tvalid_1's amex: 0.786325\n",
      "[1100]\ttraining's binary_logloss: 0.181536\ttraining's amex: 0.848927\tvalid_1's binary_logloss: 0.220441\tvalid_1's amex: 0.786383\n",
      "[1200]\ttraining's binary_logloss: 0.178664\ttraining's amex: 0.853239\tvalid_1's binary_logloss: 0.220366\tvalid_1's amex: 0.786585\n",
      "[1300]\ttraining's binary_logloss: 0.17567\ttraining's amex: 0.858108\tvalid_1's binary_logloss: 0.22033\tvalid_1's amex: 0.786708\n",
      "[1400]\ttraining's binary_logloss: 0.172913\ttraining's amex: 0.862714\tvalid_1's binary_logloss: 0.220356\tvalid_1's amex: 0.786262\n",
      "[1500]\ttraining's binary_logloss: 0.170578\ttraining's amex: 0.866853\tvalid_1's binary_logloss: 0.220357\tvalid_1's amex: 0.786563\n",
      "[1600]\ttraining's binary_logloss: 0.168345\ttraining's amex: 0.870772\tvalid_1's binary_logloss: 0.220392\tvalid_1's amex: 0.786581\n",
      "[1700]\ttraining's binary_logloss: 0.165944\ttraining's amex: 0.87506\tvalid_1's binary_logloss: 0.220392\tvalid_1's amex: 0.786441\n",
      "[1800]\ttraining's binary_logloss: 0.163553\ttraining's amex: 0.879383\tvalid_1's binary_logloss: 0.220393\tvalid_1's amex: 0.786474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 18:43:55,792]\u001b[0m Trial 3 finished with value: 0.5800413263118607 and parameters: {'learning_rate': 0.02004087187654156, 'n_estimators': 1810, 'max_depth': 12, 'min_child_samples': 2279, 'max_bin': 387, 'num_leaves': 455}. Best is trial 0 with value: 0.5783413441846182.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.260288\ttraining's amex: 0.766337\tvalid_1's binary_logloss: 0.264172\tvalid_1's amex: 0.753202\n",
      "[200]\ttraining's binary_logloss: 0.228559\ttraining's amex: 0.782778\tvalid_1's binary_logloss: 0.235397\tvalid_1's amex: 0.767546\n",
      "[300]\ttraining's binary_logloss: 0.219296\ttraining's amex: 0.792669\tvalid_1's binary_logloss: 0.228646\tvalid_1's amex: 0.77503\n",
      "[400]\ttraining's binary_logloss: 0.214507\ttraining's amex: 0.798413\tvalid_1's binary_logloss: 0.225613\tvalid_1's amex: 0.779227\n",
      "[500]\ttraining's binary_logloss: 0.210793\ttraining's amex: 0.803694\tvalid_1's binary_logloss: 0.22368\tvalid_1's amex: 0.782253\n",
      "[600]\ttraining's binary_logloss: 0.208159\ttraining's amex: 0.807739\tvalid_1's binary_logloss: 0.222704\tvalid_1's amex: 0.783402\n",
      "[700]\ttraining's binary_logloss: 0.205894\ttraining's amex: 0.811022\tvalid_1's binary_logloss: 0.222065\tvalid_1's amex: 0.784563\n",
      "[800]\ttraining's binary_logloss: 0.203894\ttraining's amex: 0.814221\tvalid_1's binary_logloss: 0.22163\tvalid_1's amex: 0.78525\n",
      "[900]\ttraining's binary_logloss: 0.202102\ttraining's amex: 0.817104\tvalid_1's binary_logloss: 0.22135\tvalid_1's amex: 0.785522\n",
      "[1000]\ttraining's binary_logloss: 0.200373\ttraining's amex: 0.819751\tvalid_1's binary_logloss: 0.221107\tvalid_1's amex: 0.786162\n",
      "[1100]\ttraining's binary_logloss: 0.198685\ttraining's amex: 0.82256\tvalid_1's binary_logloss: 0.220922\tvalid_1's amex: 0.786167\n",
      "[1200]\ttraining's binary_logloss: 0.196931\ttraining's amex: 0.825136\tvalid_1's binary_logloss: 0.220768\tvalid_1's amex: 0.786099\n",
      "[1300]\ttraining's binary_logloss: 0.19535\ttraining's amex: 0.827631\tvalid_1's binary_logloss: 0.220651\tvalid_1's amex: 0.786648\n",
      "[1400]\ttraining's binary_logloss: 0.193732\ttraining's amex: 0.830227\tvalid_1's binary_logloss: 0.220596\tvalid_1's amex: 0.786543\n",
      "[1500]\ttraining's binary_logloss: 0.192165\ttraining's amex: 0.832804\tvalid_1's binary_logloss: 0.220537\tvalid_1's amex: 0.786356\n",
      "[1600]\ttraining's binary_logloss: 0.190797\ttraining's amex: 0.835324\tvalid_1's binary_logloss: 0.220471\tvalid_1's amex: 0.786153\n",
      "[1700]\ttraining's binary_logloss: 0.189296\ttraining's amex: 0.83782\tvalid_1's binary_logloss: 0.22045\tvalid_1's amex: 0.786647\n",
      "[1800]\ttraining's binary_logloss: 0.187801\ttraining's amex: 0.839948\tvalid_1's binary_logloss: 0.220442\tvalid_1's amex: 0.786632\n",
      "[1900]\ttraining's binary_logloss: 0.186395\ttraining's amex: 0.842403\tvalid_1's binary_logloss: 0.22043\tvalid_1's amex: 0.786555\n",
      "[2000]\ttraining's binary_logloss: 0.184949\ttraining's amex: 0.844659\tvalid_1's binary_logloss: 0.22043\tvalid_1's amex: 0.786114\n",
      "[2100]\ttraining's binary_logloss: 0.183448\ttraining's amex: 0.847138\tvalid_1's binary_logloss: 0.220429\tvalid_1's amex: 0.786623\n",
      "[2200]\ttraining's binary_logloss: 0.182124\ttraining's amex: 0.849441\tvalid_1's binary_logloss: 0.22045\tvalid_1's amex: 0.786259\n",
      "[2300]\ttraining's binary_logloss: 0.180718\ttraining's amex: 0.85206\tvalid_1's binary_logloss: 0.220475\tvalid_1's amex: 0.785981\n",
      "[2400]\ttraining's binary_logloss: 0.17935\ttraining's amex: 0.854197\tvalid_1's binary_logloss: 0.22048\tvalid_1's amex: 0.785163\n",
      "[2500]\ttraining's binary_logloss: 0.177993\ttraining's amex: 0.856309\tvalid_1's binary_logloss: 0.220478\tvalid_1's amex: 0.785694\n",
      "[2600]\ttraining's binary_logloss: 0.176844\ttraining's amex: 0.858233\tvalid_1's binary_logloss: 0.22053\tvalid_1's amex: 0.785132\n",
      "[2700]\ttraining's binary_logloss: 0.175748\ttraining's amex: 0.860219\tvalid_1's binary_logloss: 0.220572\tvalid_1's amex: 0.785457\n",
      "[2800]\ttraining's binary_logloss: 0.174521\ttraining's amex: 0.862481\tvalid_1's binary_logloss: 0.220608\tvalid_1's amex: 0.785322\n",
      "[2900]\ttraining's binary_logloss: 0.173326\ttraining's amex: 0.864651\tvalid_1's binary_logloss: 0.220627\tvalid_1's amex: 0.785486\n",
      "[3000]\ttraining's binary_logloss: 0.172101\ttraining's amex: 0.866709\tvalid_1's binary_logloss: 0.220656\tvalid_1's amex: 0.785815\n",
      "[3100]\ttraining's binary_logloss: 0.170969\ttraining's amex: 0.868664\tvalid_1's binary_logloss: 0.220699\tvalid_1's amex: 0.785886\n",
      "[3200]\ttraining's binary_logloss: 0.16976\ttraining's amex: 0.870852\tvalid_1's binary_logloss: 0.220697\tvalid_1's amex: 0.786144\n",
      "[3300]\ttraining's binary_logloss: 0.168795\ttraining's amex: 0.872229\tvalid_1's binary_logloss: 0.220718\tvalid_1's amex: 0.785764\n",
      "[3400]\ttraining's binary_logloss: 0.167557\ttraining's amex: 0.874479\tvalid_1's binary_logloss: 0.220771\tvalid_1's amex: 0.785286\n",
      "[3500]\ttraining's binary_logloss: 0.166397\ttraining's amex: 0.876532\tvalid_1's binary_logloss: 0.220839\tvalid_1's amex: 0.785497\n",
      "[3600]\ttraining's binary_logloss: 0.165166\ttraining's amex: 0.878674\tvalid_1's binary_logloss: 0.220895\tvalid_1's amex: 0.785376\n",
      "[3700]\ttraining's binary_logloss: 0.163993\ttraining's amex: 0.880835\tvalid_1's binary_logloss: 0.220937\tvalid_1's amex: 0.785195\n",
      "[3800]\ttraining's binary_logloss: 0.163034\ttraining's amex: 0.882388\tvalid_1's binary_logloss: 0.220979\tvalid_1's amex: 0.785502\n",
      "[3900]\ttraining's binary_logloss: 0.161798\ttraining's amex: 0.884523\tvalid_1's binary_logloss: 0.221046\tvalid_1's amex: 0.784958\n",
      "[4000]\ttraining's binary_logloss: 0.160653\ttraining's amex: 0.886704\tvalid_1's binary_logloss: 0.221111\tvalid_1's amex: 0.784961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 19:24:33,031]\u001b[0m Trial 4 finished with value: 0.5779326657441259 and parameters: {'learning_rate': 0.020834315611529482, 'n_estimators': 4034, 'max_depth': 8, 'min_child_samples': 3028, 'max_bin': 478, 'num_leaves': 168}. Best is trial 4 with value: 0.5779326657441259.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.250971\ttraining's amex: 0.765045\tvalid_1's binary_logloss: 0.254599\tvalid_1's amex: 0.75325\n",
      "[200]\ttraining's binary_logloss: 0.22744\ttraining's amex: 0.782315\tvalid_1's binary_logloss: 0.233528\tvalid_1's amex: 0.768234\n",
      "[300]\ttraining's binary_logloss: 0.220686\ttraining's amex: 0.7908\tvalid_1's binary_logloss: 0.228368\tvalid_1's amex: 0.774825\n",
      "[400]\ttraining's binary_logloss: 0.216435\ttraining's amex: 0.796001\tvalid_1's binary_logloss: 0.22542\tvalid_1's amex: 0.779808\n",
      "[500]\ttraining's binary_logloss: 0.213664\ttraining's amex: 0.799555\tvalid_1's binary_logloss: 0.223823\tvalid_1's amex: 0.782318\n",
      "[600]\ttraining's binary_logloss: 0.211533\ttraining's amex: 0.802619\tvalid_1's binary_logloss: 0.222882\tvalid_1's amex: 0.783752\n",
      "[700]\ttraining's binary_logloss: 0.209797\ttraining's amex: 0.805498\tvalid_1's binary_logloss: 0.222275\tvalid_1's amex: 0.78469\n",
      "[800]\ttraining's binary_logloss: 0.20825\ttraining's amex: 0.807851\tvalid_1's binary_logloss: 0.221879\tvalid_1's amex: 0.784477\n",
      "[900]\ttraining's binary_logloss: 0.206738\ttraining's amex: 0.810411\tvalid_1's binary_logloss: 0.221542\tvalid_1's amex: 0.785025\n",
      "[1000]\ttraining's binary_logloss: 0.205363\ttraining's amex: 0.812629\tvalid_1's binary_logloss: 0.221239\tvalid_1's amex: 0.785918\n",
      "[1100]\ttraining's binary_logloss: 0.203963\ttraining's amex: 0.814864\tvalid_1's binary_logloss: 0.221027\tvalid_1's amex: 0.78562\n",
      "[1200]\ttraining's binary_logloss: 0.202813\ttraining's amex: 0.816514\tvalid_1's binary_logloss: 0.220914\tvalid_1's amex: 0.786445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 19:36:56,406]\u001b[0m Trial 5 finished with value: 0.5805481552925612 and parameters: {'learning_rate': 0.026586160837889786, 'n_estimators': 1267, 'max_depth': 6, 'min_child_samples': 3898, 'max_bin': 590, 'num_leaves': 852}. Best is trial 4 with value: 0.5779326657441259.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.277791\ttraining's amex: 0.762775\tvalid_1's binary_logloss: 0.281177\tvalid_1's amex: 0.750424\n",
      "[200]\ttraining's binary_logloss: 0.234297\ttraining's amex: 0.780014\tvalid_1's binary_logloss: 0.240751\tvalid_1's amex: 0.763052\n",
      "[300]\ttraining's binary_logloss: 0.220752\ttraining's amex: 0.791896\tvalid_1's binary_logloss: 0.230238\tvalid_1's amex: 0.772645\n",
      "[400]\ttraining's binary_logloss: 0.213544\ttraining's amex: 0.800491\tvalid_1's binary_logloss: 0.226092\tvalid_1's amex: 0.777864\n",
      "[500]\ttraining's binary_logloss: 0.208394\ttraining's amex: 0.807023\tvalid_1's binary_logloss: 0.22372\tvalid_1's amex: 0.781612\n",
      "[600]\ttraining's binary_logloss: 0.204287\ttraining's amex: 0.812559\tvalid_1's binary_logloss: 0.222442\tvalid_1's amex: 0.784174\n",
      "[700]\ttraining's binary_logloss: 0.200975\ttraining's amex: 0.817681\tvalid_1's binary_logloss: 0.221726\tvalid_1's amex: 0.785864\n",
      "[800]\ttraining's binary_logloss: 0.197904\ttraining's amex: 0.822255\tvalid_1's binary_logloss: 0.221232\tvalid_1's amex: 0.785521\n",
      "[900]\ttraining's binary_logloss: 0.194966\ttraining's amex: 0.82693\tvalid_1's binary_logloss: 0.220929\tvalid_1's amex: 0.785191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 19:47:39,429]\u001b[0m Trial 6 finished with value: 0.5799777966791264 and parameters: {'learning_rate': 0.016327356954687944, 'n_estimators': 939, 'max_depth': 13, 'min_child_samples': 2880, 'max_bin': 336, 'num_leaves': 571}. Best is trial 4 with value: 0.5779326657441259.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.321117\ttraining's amex: 0.751239\tvalid_1's binary_logloss: 0.32309\tvalid_1's amex: 0.741208\n",
      "[200]\ttraining's binary_logloss: 0.260148\ttraining's amex: 0.766261\tvalid_1's binary_logloss: 0.263808\tvalid_1's amex: 0.753346\n",
      "[300]\ttraining's binary_logloss: 0.238447\ttraining's amex: 0.77552\tvalid_1's binary_logloss: 0.243653\tvalid_1's amex: 0.761277\n",
      "[400]\ttraining's binary_logloss: 0.228804\ttraining's amex: 0.7826\tvalid_1's binary_logloss: 0.235317\tvalid_1's amex: 0.766887\n",
      "[500]\ttraining's binary_logloss: 0.223235\ttraining's amex: 0.788195\tvalid_1's binary_logloss: 0.231011\tvalid_1's amex: 0.771325\n",
      "[600]\ttraining's binary_logloss: 0.219705\ttraining's amex: 0.792731\tvalid_1's binary_logloss: 0.228583\tvalid_1's amex: 0.774995\n",
      "[700]\ttraining's binary_logloss: 0.217102\ttraining's amex: 0.795559\tvalid_1's binary_logloss: 0.226868\tvalid_1's amex: 0.778056\n",
      "[800]\ttraining's binary_logloss: 0.214833\ttraining's amex: 0.798593\tvalid_1's binary_logloss: 0.225455\tvalid_1's amex: 0.77986\n",
      "[900]\ttraining's binary_logloss: 0.2129\ttraining's amex: 0.800842\tvalid_1's binary_logloss: 0.224389\tvalid_1's amex: 0.781162\n",
      "[1000]\ttraining's binary_logloss: 0.211298\ttraining's amex: 0.802977\tvalid_1's binary_logloss: 0.223612\tvalid_1's amex: 0.781964\n",
      "[1100]\ttraining's binary_logloss: 0.20991\ttraining's amex: 0.804971\tvalid_1's binary_logloss: 0.223052\tvalid_1's amex: 0.783089\n",
      "[1200]\ttraining's binary_logloss: 0.208648\ttraining's amex: 0.806685\tvalid_1's binary_logloss: 0.222582\tvalid_1's amex: 0.784164\n",
      "[1300]\ttraining's binary_logloss: 0.2075\ttraining's amex: 0.807983\tvalid_1's binary_logloss: 0.222229\tvalid_1's amex: 0.784881\n",
      "[1400]\ttraining's binary_logloss: 0.206413\ttraining's amex: 0.809605\tvalid_1's binary_logloss: 0.221932\tvalid_1's amex: 0.785286\n",
      "[1500]\ttraining's binary_logloss: 0.205377\ttraining's amex: 0.811548\tvalid_1's binary_logloss: 0.221703\tvalid_1's amex: 0.785873\n",
      "[1600]\ttraining's binary_logloss: 0.204391\ttraining's amex: 0.813225\tvalid_1's binary_logloss: 0.221506\tvalid_1's amex: 0.785819\n",
      "[1700]\ttraining's binary_logloss: 0.203424\ttraining's amex: 0.814986\tvalid_1's binary_logloss: 0.221332\tvalid_1's amex: 0.785905\n",
      "[1800]\ttraining's binary_logloss: 0.202532\ttraining's amex: 0.816399\tvalid_1's binary_logloss: 0.22117\tvalid_1's amex: 0.785733\n",
      "[1900]\ttraining's binary_logloss: 0.20164\ttraining's amex: 0.818056\tvalid_1's binary_logloss: 0.221034\tvalid_1's amex: 0.786582\n",
      "[2000]\ttraining's binary_logloss: 0.200809\ttraining's amex: 0.819375\tvalid_1's binary_logloss: 0.220933\tvalid_1's amex: 0.786788\n",
      "[2100]\ttraining's binary_logloss: 0.199947\ttraining's amex: 0.820465\tvalid_1's binary_logloss: 0.220855\tvalid_1's amex: 0.78648\n",
      "[2200]\ttraining's binary_logloss: 0.199126\ttraining's amex: 0.821771\tvalid_1's binary_logloss: 0.220755\tvalid_1's amex: 0.787023\n",
      "[2300]\ttraining's binary_logloss: 0.198274\ttraining's amex: 0.823026\tvalid_1's binary_logloss: 0.220689\tvalid_1's amex: 0.786815\n",
      "[2400]\ttraining's binary_logloss: 0.197428\ttraining's amex: 0.82449\tvalid_1's binary_logloss: 0.22062\tvalid_1's amex: 0.787072\n",
      "[2500]\ttraining's binary_logloss: 0.196668\ttraining's amex: 0.825812\tvalid_1's binary_logloss: 0.22056\tvalid_1's amex: 0.78659\n",
      "[2600]\ttraining's binary_logloss: 0.195923\ttraining's amex: 0.826807\tvalid_1's binary_logloss: 0.220534\tvalid_1's amex: 0.786643\n",
      "[2700]\ttraining's binary_logloss: 0.195184\ttraining's amex: 0.827983\tvalid_1's binary_logloss: 0.220466\tvalid_1's amex: 0.786983\n",
      "[2800]\ttraining's binary_logloss: 0.194467\ttraining's amex: 0.829039\tvalid_1's binary_logloss: 0.220437\tvalid_1's amex: 0.78706\n",
      "[2900]\ttraining's binary_logloss: 0.19377\ttraining's amex: 0.830228\tvalid_1's binary_logloss: 0.220422\tvalid_1's amex: 0.787004\n",
      "[3000]\ttraining's binary_logloss: 0.192961\ttraining's amex: 0.831465\tvalid_1's binary_logloss: 0.220351\tvalid_1's amex: 0.786864\n",
      "[3100]\ttraining's binary_logloss: 0.192214\ttraining's amex: 0.832694\tvalid_1's binary_logloss: 0.220312\tvalid_1's amex: 0.786607\n",
      "[3200]\ttraining's binary_logloss: 0.191428\ttraining's amex: 0.833857\tvalid_1's binary_logloss: 0.22027\tvalid_1's amex: 0.787024\n",
      "[3300]\ttraining's binary_logloss: 0.190714\ttraining's amex: 0.835107\tvalid_1's binary_logloss: 0.220251\tvalid_1's amex: 0.787138\n",
      "[3400]\ttraining's binary_logloss: 0.189975\ttraining's amex: 0.836071\tvalid_1's binary_logloss: 0.220238\tvalid_1's amex: 0.787123\n",
      "[3500]\ttraining's binary_logloss: 0.189258\ttraining's amex: 0.837181\tvalid_1's binary_logloss: 0.220226\tvalid_1's amex: 0.78715\n",
      "[3600]\ttraining's binary_logloss: 0.188604\ttraining's amex: 0.838167\tvalid_1's binary_logloss: 0.220208\tvalid_1's amex: 0.78718\n",
      "[3700]\ttraining's binary_logloss: 0.187908\ttraining's amex: 0.839294\tvalid_1's binary_logloss: 0.220198\tvalid_1's amex: 0.786911\n",
      "[3800]\ttraining's binary_logloss: 0.187236\ttraining's amex: 0.840371\tvalid_1's binary_logloss: 0.220197\tvalid_1's amex: 0.787291\n",
      "[3900]\ttraining's binary_logloss: 0.186561\ttraining's amex: 0.841637\tvalid_1's binary_logloss: 0.220201\tvalid_1's amex: 0.786933\n",
      "[4000]\ttraining's binary_logloss: 0.185853\ttraining's amex: 0.84294\tvalid_1's binary_logloss: 0.220213\tvalid_1's amex: 0.786909\n",
      "[4100]\ttraining's binary_logloss: 0.185138\ttraining's amex: 0.844196\tvalid_1's binary_logloss: 0.220212\tvalid_1's amex: 0.786912\n",
      "[4200]\ttraining's binary_logloss: 0.18447\ttraining's amex: 0.845397\tvalid_1's binary_logloss: 0.2202\tvalid_1's amex: 0.786434\n",
      "[4300]\ttraining's binary_logloss: 0.183889\ttraining's amex: 0.846266\tvalid_1's binary_logloss: 0.220186\tvalid_1's amex: 0.786755\n",
      "[4400]\ttraining's binary_logloss: 0.183269\ttraining's amex: 0.847205\tvalid_1's binary_logloss: 0.220181\tvalid_1's amex: 0.786653\n",
      "[4500]\ttraining's binary_logloss: 0.182619\ttraining's amex: 0.84832\tvalid_1's binary_logloss: 0.22017\tvalid_1's amex: 0.786594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 20:34:07,844]\u001b[0m Trial 7 finished with value: 0.5799924633301632 and parameters: {'learning_rate': 0.010569064414047038, 'n_estimators': 4592, 'max_depth': 8, 'min_child_samples': 3325, 'max_bin': 393, 'num_leaves': 593}. Best is trial 4 with value: 0.5779326657441259.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.251101\ttraining's amex: 0.769823\tvalid_1's binary_logloss: 0.255399\tvalid_1's amex: 0.756191\n",
      "[200]\ttraining's binary_logloss: 0.222806\ttraining's amex: 0.78884\tvalid_1's binary_logloss: 0.231181\tvalid_1's amex: 0.771983\n",
      "[300]\ttraining's binary_logloss: 0.212046\ttraining's amex: 0.801581\tvalid_1's binary_logloss: 0.225054\tvalid_1's amex: 0.78036\n",
      "[400]\ttraining's binary_logloss: 0.205253\ttraining's amex: 0.810707\tvalid_1's binary_logloss: 0.22259\tvalid_1's amex: 0.783757\n",
      "[500]\ttraining's binary_logloss: 0.20019\ttraining's amex: 0.8179\tvalid_1's binary_logloss: 0.221505\tvalid_1's amex: 0.785797\n",
      "[600]\ttraining's binary_logloss: 0.195584\ttraining's amex: 0.825284\tvalid_1's binary_logloss: 0.220961\tvalid_1's amex: 0.786774\n",
      "[700]\ttraining's binary_logloss: 0.191426\ttraining's amex: 0.830902\tvalid_1's binary_logloss: 0.220577\tvalid_1's amex: 0.787718\n",
      "[800]\ttraining's binary_logloss: 0.187489\ttraining's amex: 0.837147\tvalid_1's binary_logloss: 0.220391\tvalid_1's amex: 0.78758\n",
      "[900]\ttraining's binary_logloss: 0.183726\ttraining's amex: 0.843246\tvalid_1's binary_logloss: 0.220242\tvalid_1's amex: 0.788144\n",
      "[1000]\ttraining's binary_logloss: 0.179963\ttraining's amex: 0.849368\tvalid_1's binary_logloss: 0.220167\tvalid_1's amex: 0.787675\n",
      "[1100]\ttraining's binary_logloss: 0.176482\ttraining's amex: 0.855299\tvalid_1's binary_logloss: 0.220128\tvalid_1's amex: 0.788115\n",
      "[1200]\ttraining's binary_logloss: 0.173229\ttraining's amex: 0.860609\tvalid_1's binary_logloss: 0.220139\tvalid_1's amex: 0.788123\n",
      "[1300]\ttraining's binary_logloss: 0.169948\ttraining's amex: 0.866303\tvalid_1's binary_logloss: 0.22016\tvalid_1's amex: 0.788082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 20:49:28,927]\u001b[0m Trial 8 finished with value: 0.5809740241566005 and parameters: {'learning_rate': 0.02410649590217162, 'n_estimators': 1332, 'max_depth': 16, 'min_child_samples': 3551, 'max_bin': 582, 'num_leaves': 929}. Best is trial 4 with value: 0.5779326657441259.\u001b[0m\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/sohan/anaconda3/envs/kaggle-amex/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.250045\ttraining's amex: 0.766044\tvalid_1's binary_logloss: 0.254053\tvalid_1's amex: 0.754436\n",
      "[200]\ttraining's binary_logloss: 0.226156\ttraining's amex: 0.784186\tvalid_1's binary_logloss: 0.232847\tvalid_1's amex: 0.769605\n",
      "[300]\ttraining's binary_logloss: 0.219133\ttraining's amex: 0.792702\tvalid_1's binary_logloss: 0.227688\tvalid_1's amex: 0.775949\n",
      "[400]\ttraining's binary_logloss: 0.214957\ttraining's amex: 0.798302\tvalid_1's binary_logloss: 0.22497\tvalid_1's amex: 0.779851\n",
      "[500]\ttraining's binary_logloss: 0.212138\ttraining's amex: 0.802422\tvalid_1's binary_logloss: 0.223447\tvalid_1's amex: 0.782305\n",
      "[600]\ttraining's binary_logloss: 0.209837\ttraining's amex: 0.805925\tvalid_1's binary_logloss: 0.222527\tvalid_1's amex: 0.783772\n",
      "[700]\ttraining's binary_logloss: 0.207849\ttraining's amex: 0.808705\tvalid_1's binary_logloss: 0.221948\tvalid_1's amex: 0.784507\n",
      "[800]\ttraining's binary_logloss: 0.206028\ttraining's amex: 0.811535\tvalid_1's binary_logloss: 0.221522\tvalid_1's amex: 0.78576\n",
      "[900]\ttraining's binary_logloss: 0.204333\ttraining's amex: 0.814059\tvalid_1's binary_logloss: 0.221249\tvalid_1's amex: 0.786514\n",
      "[1000]\ttraining's binary_logloss: 0.202778\ttraining's amex: 0.816623\tvalid_1's binary_logloss: 0.221008\tvalid_1's amex: 0.786732\n",
      "[1100]\ttraining's binary_logloss: 0.201324\ttraining's amex: 0.819267\tvalid_1's binary_logloss: 0.220856\tvalid_1's amex: 0.787294\n",
      "[1200]\ttraining's binary_logloss: 0.199921\ttraining's amex: 0.821652\tvalid_1's binary_logloss: 0.220708\tvalid_1's amex: 0.78729\n",
      "[1300]\ttraining's binary_logloss: 0.198658\ttraining's amex: 0.823813\tvalid_1's binary_logloss: 0.220606\tvalid_1's amex: 0.787688\n",
      "[1400]\ttraining's binary_logloss: 0.197275\ttraining's amex: 0.825937\tvalid_1's binary_logloss: 0.220546\tvalid_1's amex: 0.78733\n",
      "[1500]\ttraining's binary_logloss: 0.195957\ttraining's amex: 0.827602\tvalid_1's binary_logloss: 0.220441\tvalid_1's amex: 0.787201\n",
      "[1600]\ttraining's binary_logloss: 0.194613\ttraining's amex: 0.829746\tvalid_1's binary_logloss: 0.220397\tvalid_1's amex: 0.787915\n",
      "[1700]\ttraining's binary_logloss: 0.19328\ttraining's amex: 0.831989\tvalid_1's binary_logloss: 0.220326\tvalid_1's amex: 0.787311\n",
      "[1800]\ttraining's binary_logloss: 0.191968\ttraining's amex: 0.833917\tvalid_1's binary_logloss: 0.220277\tvalid_1's amex: 0.787499\n",
      "[1900]\ttraining's binary_logloss: 0.19069\ttraining's amex: 0.835884\tvalid_1's binary_logloss: 0.220242\tvalid_1's amex: 0.787388\n",
      "[2000]\ttraining's binary_logloss: 0.189426\ttraining's amex: 0.837918\tvalid_1's binary_logloss: 0.220225\tvalid_1's amex: 0.787143\n",
      "[2100]\ttraining's binary_logloss: 0.188166\ttraining's amex: 0.840301\tvalid_1's binary_logloss: 0.220224\tvalid_1's amex: 0.786745\n",
      "[2200]\ttraining's binary_logloss: 0.18701\ttraining's amex: 0.842302\tvalid_1's binary_logloss: 0.22022\tvalid_1's amex: 0.786497\n",
      "[2300]\ttraining's binary_logloss: 0.185858\ttraining's amex: 0.844403\tvalid_1's binary_logloss: 0.220195\tvalid_1's amex: 0.786952\n",
      "[2400]\ttraining's binary_logloss: 0.184678\ttraining's amex: 0.846459\tvalid_1's binary_logloss: 0.220157\tvalid_1's amex: 0.787286\n",
      "[2500]\ttraining's binary_logloss: 0.183483\ttraining's amex: 0.848862\tvalid_1's binary_logloss: 0.220156\tvalid_1's amex: 0.787334\n",
      "[2600]\ttraining's binary_logloss: 0.182296\ttraining's amex: 0.850627\tvalid_1's binary_logloss: 0.220173\tvalid_1's amex: 0.787395\n",
      "[2700]\ttraining's binary_logloss: 0.181281\ttraining's amex: 0.852215\tvalid_1's binary_logloss: 0.220194\tvalid_1's amex: 0.787177\n",
      "[2800]\ttraining's binary_logloss: 0.180192\ttraining's amex: 0.854015\tvalid_1's binary_logloss: 0.220185\tvalid_1's amex: 0.787097\n",
      "[2900]\ttraining's binary_logloss: 0.17919\ttraining's amex: 0.855661\tvalid_1's binary_logloss: 0.220234\tvalid_1's amex: 0.787168\n",
      "[3000]\ttraining's binary_logloss: 0.178095\ttraining's amex: 0.857435\tvalid_1's binary_logloss: 0.220279\tvalid_1's amex: 0.787366\n",
      "[3100]\ttraining's binary_logloss: 0.176919\ttraining's amex: 0.859428\tvalid_1's binary_logloss: 0.220302\tvalid_1's amex: 0.787384\n",
      "[3200]\ttraining's binary_logloss: 0.175849\ttraining's amex: 0.861137\tvalid_1's binary_logloss: 0.220343\tvalid_1's amex: 0.78731\n",
      "[3300]\ttraining's binary_logloss: 0.174749\ttraining's amex: 0.86307\tvalid_1's binary_logloss: 0.220395\tvalid_1's amex: 0.787526\n",
      "[3400]\ttraining's binary_logloss: 0.173643\ttraining's amex: 0.864841\tvalid_1's binary_logloss: 0.220442\tvalid_1's amex: 0.787344\n",
      "[3500]\ttraining's binary_logloss: 0.172492\ttraining's amex: 0.866986\tvalid_1's binary_logloss: 0.220484\tvalid_1's amex: 0.787396\n",
      "[3600]\ttraining's binary_logloss: 0.171439\ttraining's amex: 0.869009\tvalid_1's binary_logloss: 0.220578\tvalid_1's amex: 0.787089\n",
      "[3700]\ttraining's binary_logloss: 0.170444\ttraining's amex: 0.871008\tvalid_1's binary_logloss: 0.220613\tvalid_1's amex: 0.787037\n",
      "[3800]\ttraining's binary_logloss: 0.169389\ttraining's amex: 0.872794\tvalid_1's binary_logloss: 0.22062\tvalid_1's amex: 0.787187\n",
      "[3900]\ttraining's binary_logloss: 0.168348\ttraining's amex: 0.874657\tvalid_1's binary_logloss: 0.220645\tvalid_1's amex: 0.787417\n",
      "[4000]\ttraining's binary_logloss: 0.167227\ttraining's amex: 0.876816\tvalid_1's binary_logloss: 0.220679\tvalid_1's amex: 0.787032\n",
      "[4100]\ttraining's binary_logloss: 0.166245\ttraining's amex: 0.878659\tvalid_1's binary_logloss: 0.22072\tvalid_1's amex: 0.787253\n",
      "[4200]\ttraining's binary_logloss: 0.165353\ttraining's amex: 0.880028\tvalid_1's binary_logloss: 0.220781\tvalid_1's amex: 0.786814\n",
      "[4300]\ttraining's binary_logloss: 0.164328\ttraining's amex: 0.881895\tvalid_1's binary_logloss: 0.220799\tvalid_1's amex: 0.786981\n",
      "[4400]\ttraining's binary_logloss: 0.163382\ttraining's amex: 0.883416\tvalid_1's binary_logloss: 0.220844\tvalid_1's amex: 0.7876\n",
      "[4500]\ttraining's binary_logloss: 0.162539\ttraining's amex: 0.884964\tvalid_1's binary_logloss: 0.220913\tvalid_1's amex: 0.786991\n",
      "[4600]\ttraining's binary_logloss: 0.161677\ttraining's amex: 0.886225\tvalid_1's binary_logloss: 0.220981\tvalid_1's amex: 0.787074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-30 21:34:31,926]\u001b[0m Trial 9 finished with value: 0.5797399504602833 and parameters: {'learning_rate': 0.02617665509704007, 'n_estimators': 4649, 'max_depth': 6, 'min_child_samples': 2392, 'max_bin': 313, 'num_leaves': 418}. Best is trial 4 with value: 0.5779326657441259.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5779326657441259\n",
      "Best trial {'learning_rate': 0.020834315611529482, 'n_estimators': 4034, 'max_depth': 8, 'min_child_samples': 3028, 'max_bin': 478, 'num_leaves': 168}\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name = 'lgbm_parameter_opt',\n",
    "    direction = 'minimize',\n",
    "    sampler = sampler,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Best Score:\",study.best_value)\n",
    "print(\"Best trial\",study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INFERENCE:\n",
    "    sub = pd.DataFrame({'customer_ID': test.index,\n",
    "                        'prediction': np.mean(y_pred_list, axis=0)})\n",
    "    sub.to_csv('results/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3b1f614b1596e3dfeedccb6ee4bbb4b3c66c39d76de8dd8646131c8c60a95b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kaggle-amex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
