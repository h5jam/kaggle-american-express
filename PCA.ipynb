{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-11T11:57:29.866723Z","iopub.execute_input":"2022-06-11T11:57:29.867544Z","iopub.status.idle":"2022-06-11T11:57:29.903826Z","shell.execute_reply.started":"2022-06-11T11:57:29.867410Z","shell.execute_reply":"2022-06-11T11:57:29.902523Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n\nThe dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:\n\nD_* = Delinquency variables\nS_* = Spend variables\nP_* = Payment variables\nB_* = Balance variables\nR_* = Risk variables\nwith the following features being categorical:\n\n['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n\nYour task is to predict, for each customer_ID, the probability of a future payment default (target = 1).\n\nNote that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom pathlib import Path\nimport seaborn as sb\nimport random\nimport gc\nsb.color_palette(\"Spectral\", as_cmap=True)\n\n\n\nTRAIN_DATA_PATH = \"/kaggle/input/parquet-files-amexdefault-prediction/\"\nTRAIN_FILE = \"train_data\"\nTRAIN_LABELS = \"train_labels\"\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-11T11:57:36.251967Z","iopub.execute_input":"2022-06-11T11:57:36.252373Z","iopub.status.idle":"2022-06-11T11:57:36.927719Z","shell.execute_reply.started":"2022-06-11T11:57:36.252342Z","shell.execute_reply":"2022-06-11T11:57:36.926797Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Convert .csv to feather and save\nCSV_DATA_PATH = \"/kaggle/input/amex-default-prediction/\"\ndf = pd.read_csv(os.path.join(CSV_DATA_PATH,TRAIN_LABELS+\".csv\"))\npath=os.path.join(\"/kaggle/working/\",TRAIN_LABELS+\".ftr\")\nif Path(path).exists():\n   os.remove(\"/kaggle/working/train_labels.ftr\")\nprint(f\"Location of Feather File {path}\")\ndf.to_feather(path)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T11:57:39.131044Z","iopub.execute_input":"2022-06-11T11:57:39.131591Z","iopub.status.idle":"2022-06-11T11:57:40.426882Z","shell.execute_reply.started":"2022-06-11T11:57:39.131546Z","shell.execute_reply":"2022-06-11T11:57:40.425744Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def load_data(fpath,fname,flabels):\n    file_path = os.path.join(fpath,fname+\".ftr\")\n    file_path = Path(file_path)\n    file_labels = Path(os.path.join(\"/kaggle/working/\",flabels+\".ftr\"))\n    if file_path.exists() and file_labels.exists():\n        print(f\"{file_path}, and {file_labels} are available\")\n        df1 = pd.read_feather(file_path,use_threads=True)\n        df2 = pd.read_feather(file_labels,use_threads=True)\n        \n        return df1,df2\n    else:\n        print(\"No Such File\")\n        return\n ","metadata":{"execution":{"iopub.status.busy":"2022-06-11T11:57:41.817643Z","iopub.execute_input":"2022-06-11T11:57:41.818284Z","iopub.status.idle":"2022-06-11T11:57:41.830769Z","shell.execute_reply.started":"2022-06-11T11:57:41.818246Z","shell.execute_reply":"2022-06-11T11:57:41.829244Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_x,train_y = load_data(TRAIN_DATA_PATH,TRAIN_FILE,TRAIN_LABELS )\ntrain_x.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T11:57:43.589858Z","iopub.execute_input":"2022-06-11T11:57:43.590233Z","iopub.status.idle":"2022-06-11T11:58:03.606335Z","shell.execute_reply.started":"2022-06-11T11:57:43.590202Z","shell.execute_reply":"2022-06-11T11:58:03.605100Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"categorical_var = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\ndiscard = [\"customer_ID\",\"S_2\"] + categorical_var\nnumeric_cols = list(set(train_x.columns)-set(discard))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T11:58:06.292881Z","iopub.execute_input":"2022-06-11T11:58:06.293327Z","iopub.status.idle":"2022-06-11T11:58:06.300005Z","shell.execute_reply.started":"2022-06-11T11:58:06.293286Z","shell.execute_reply":"2022-06-11T11:58:06.298781Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfor col in train_x.columns:\n    if col in categorical_var:\n        print(col)\n\n#num_cols = train_x._get_numeric_data().columns\n#cat_var = list(set(train_x.columns)-set(num_cols))\n#cat_var","metadata":{"execution":{"iopub.status.busy":"2022-06-11T11:58:08.527894Z","iopub.execute_input":"2022-06-11T11:58:08.528310Z","iopub.status.idle":"2022-06-11T11:58:08.535481Z","shell.execute_reply.started":"2022-06-11T11:58:08.528279Z","shell.execute_reply":"2022-06-11T11:58:08.534391Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Let us conduct the Chi-Square Test to find the correlation between the categorical variables.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom scipy.stats import chi2_contingency\n'''\nH0: Categorical variables are not correlated\nH1: Categorical variables are highly correlated\n'''\ndrop_catvar=[]\ncols = [i for i in train_x.columns.to_list()]\nfor i in range(len(train_x.columns)-1):\n    col1 = cols[i]\n    col2 = cols[i+1]\n   \n    if col1 in categorical_var:\n        if col2 in categorical_var:\n             \n             result = pd.crosstab(index=train_x[col1],columns=train_x[col2])\n             chi2 = chi2_contingency(result)\n                \n             if chi2[1] >= 0.05:\n                 print(f\"{col1} and {col2} are not correlated, p-value: {chi2[1]}\")\n             else:\n                 print(f\"{col1} and {col2} are correlated, p-value: {chi2[1]}\")\n             drop_catvar.append(col2)\n\ncategorical_var = list(set(categorical_var)-set(drop_catvar))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T11:58:10.656917Z","iopub.execute_input":"2022-06-11T11:58:10.657780Z","iopub.status.idle":"2022-06-11T11:58:11.689691Z","shell.execute_reply.started":"2022-06-11T11:58:10.657709Z","shell.execute_reply":"2022-06-11T11:58:11.688429Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We need to drop highly correlated data.","metadata":{}},{"cell_type":"code","source":"df_temp = pd.merge(train_x,train_y,on=[\"customer_ID\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-11T11:58:17.882880Z","iopub.execute_input":"2022-06-11T11:58:17.883701Z","iopub.status.idle":"2022-06-11T11:59:54.975976Z","shell.execute_reply.started":"2022-06-11T11:58:17.883644Z","shell.execute_reply":"2022-06-11T11:59:54.974421Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Two-Way Table - Conditional Probability for Categorical Variable\nfor col in categorical_var:\n    result = pd.crosstab(index=df_temp['target'],columns=train_x[col],normalize=\"index\",margins=True,dropna=True)\n    print(result)\n    print(\"*\"*20)\ndel df_temp\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:00:46.959368Z","iopub.execute_input":"2022-06-11T12:00:46.960213Z","iopub.status.idle":"2022-06-11T12:01:01.762476Z","shell.execute_reply.started":"2022-06-11T12:00:46.960166Z","shell.execute_reply":"2022-06-11T12:01:01.761305Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Column 'B_30' label '0.0' is influencing most the target column.Column 'B_38' label '2.0' is influencing most the target column.","metadata":{}},{"cell_type":"code","source":"print(train_x[\"D_63\"].iloc[0:5])","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:01:05.816205Z","iopub.execute_input":"2022-06-11T12:01:05.816682Z","iopub.status.idle":"2022-06-11T12:01:05.825058Z","shell.execute_reply.started":"2022-06-11T12:01:05.816646Z","shell.execute_reply":"2022-06-11T12:01:05.823735Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for col in train_x.columns:\n    if col in categorical_var:\n        dummy = pd.get_dummies(train_x[col],prefix=col)\n        train_x = train_x.join(dummy)\n        train_x.drop(col,axis=1,inplace=True)\ntrain_x.head()   ","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:01:12.212938Z","iopub.execute_input":"2022-06-11T12:01:12.213361Z","iopub.status.idle":"2022-06-11T12:02:41.246284Z","shell.execute_reply.started":"2022-06-11T12:01:12.213329Z","shell.execute_reply":"2022-06-11T12:02:41.245248Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_x[\"S_2\"]=pd.to_datetime(train_x[\"S_2\"])\n\nfor col in numeric_cols:\n           \n        try:\n            train_x[col]=pd.to_numeric(train_x[col])\n    \n        except:\n               print(\"Casting Error\")","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:02:56.731487Z","iopub.execute_input":"2022-06-11T12:02:56.732000Z","iopub.status.idle":"2022-06-11T12:02:58.439579Z","shell.execute_reply.started":"2022-06-11T12:02:56.731964Z","shell.execute_reply":"2022-06-11T12:02:58.438348Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Missing Value \ncol_value=[]\ncol_name = []\nfor col in train_x.columns:\n    missing_val = train_x[col].isna().sum()\n    missing_val_per = round(missing_val*100/len(train_x),2)\n    col_value.append(float(missing_val_per))\n    col_name.append(col)\n    #print(f\"Column {col} has {missing_val} number of missing values i.e.{missing_val_per}%\")\n\n\nmissing_val_df = pd.DataFrame({\"Col\":col_name,\"Missing Value in Percentage\":col_value})\nmissing_val_df = missing_val_df.sort_values(\"Missing Value in Percentage\",ascending=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:03:02.749395Z","iopub.execute_input":"2022-06-11T12:03:02.749908Z","iopub.status.idle":"2022-06-11T12:03:08.026313Z","shell.execute_reply.started":"2022-06-11T12:03:02.749873Z","shell.execute_reply":"2022-06-11T12:03:08.025149Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(80,50))\nplt.bar(missing_val_df[\"Col\"],missing_val_df[\"Missing Value in Percentage\"],width=0.8, color='green')\nplt.xticks(rotation=90)\nplt.xlabel(\"Column Name\", fontsize=18)\nplt.ylabel(\"Percentage\",fontsize=18)\nplt.title(\"Missing Value\",fontsize=20)\nplt.legend(\"AMEX\",fontsize=18)\nplt.show()\ndel missing_val_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:03:15.659054Z","iopub.execute_input":"2022-06-11T12:03:15.659514Z","iopub.status.idle":"2022-06-11T12:03:19.467502Z","shell.execute_reply.started":"2022-06-11T12:03:15.659478Z","shell.execute_reply":"2022-06-11T12:03:19.466555Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_x=train_x.dropna(axis=1)\ntrain_x.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:03:22.514079Z","iopub.execute_input":"2022-06-11T12:03:22.515160Z","iopub.status.idle":"2022-06-11T12:03:29.896557Z","shell.execute_reply.started":"2022-06-11T12:03:22.515117Z","shell.execute_reply":"2022-06-11T12:03:29.895454Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Number of columns\ncatcols=[]\nfor col in train_x.columns:\n    for cat_var in categorical_var:\n        if col.startswith(cat_var):\n            catcols.append(col)\n            \nnumeric_cols = list(set(train_x.columns)-set(discard)-set(catcols))\nlen(numeric_cols)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:03:34.392601Z","iopub.execute_input":"2022-06-11T12:03:34.393370Z","iopub.status.idle":"2022-06-11T12:03:34.402213Z","shell.execute_reply.started":"2022-06-11T12:03:34.393328Z","shell.execute_reply":"2022-06-11T12:03:34.401205Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Detection of Class Imbalance\ntarget = train_y.drop(\"customer_ID\",axis=1)\ntarget.tail()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:03:36.724884Z","iopub.execute_input":"2022-06-11T12:03:36.725597Z","iopub.status.idle":"2022-06-11T12:03:36.740676Z","shell.execute_reply.started":"2022-06-11T12:03:36.725563Z","shell.execute_reply":"2022-06-11T12:03:36.739566Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def class_imbalance(df):\n    yes = df[df[\"target\"]==1]\n    no = df[df[\"target\"]==0]\n    pyes = len(yes)*100/(len(no)+len(yes))\n    pno = 100-pyes\n    print(\"Percentage of Class 1:\",round(pyes,2),\"%\")\n    print(\"Percentage of Class 0:\",round(pno,2),\"%\")\n    if (pyes != pno):\n        print(\"Class Imbalance Exsists\\n\")\n    else:\n        print(\"No Class Imbalance Exsists\\n\")\n    plt.figure(figsize=(10,10))\n    xlab = [\"1\",\"0\"]\n    xpos =np.arange(len(xlab))\n    ylab=[pyes/100,pno/100]\n    plt.bar(xpos,ylab,width=0.4,alpha=0.7, color=\"green\")\n    plt.xticks(xpos,xlab)\n    plt.title(\"Class Imblance\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:05:59.783277Z","iopub.execute_input":"2022-06-11T12:05:59.783751Z","iopub.status.idle":"2022-06-11T12:05:59.794694Z","shell.execute_reply.started":"2022-06-11T12:05:59.783716Z","shell.execute_reply":"2022-06-11T12:05:59.793426Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class_imbalance(target)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:06:02.379724Z","iopub.execute_input":"2022-06-11T12:06:02.380337Z","iopub.status.idle":"2022-06-11T12:06:02.543355Z","shell.execute_reply.started":"2022-06-11T12:06:02.380303Z","shell.execute_reply":"2022-06-11T12:06:02.542081Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#Down sampling factor 3\ndef down_sampling(trainx,trainy):\n    df_merge = pd.merge(trainx,trainy,on=\"customer_ID\")\n    df_merge_1 = df_merge[df_merge['target']==1]\n    df_merge_0 = df_merge[df_merge['target']==0]\n    df_merge_dsample0 = df_merge_0.sample(n=len(df_merge_1))\n    df_dsample = pd.concat([df_merge_dsample0,df_merge_1])\n    train_y_dsample =df_dsample[\"target\"] \n    train_x_dsample = df_dsample.drop(\"target\",axis=1)\n    del df_merge,df_merge_1,df_merge_0,trainx,trainy\n    gc.collect()\n    return train_x_dsample,train_y_dsample","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:06:10.598263Z","iopub.execute_input":"2022-06-11T12:06:10.598676Z","iopub.status.idle":"2022-06-11T12:06:10.606927Z","shell.execute_reply.started":"2022-06-11T12:06:10.598644Z","shell.execute_reply":"2022-06-11T12:06:10.605620Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"trainx,trainy = down_sampling(train_x,train_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:06:12.670091Z","iopub.execute_input":"2022-06-11T12:06:12.670650Z","iopub.status.idle":"2022-06-11T12:06:33.313134Z","shell.execute_reply.started":"2022-06-11T12:06:12.670618Z","shell.execute_reply":"2022-06-11T12:06:33.311926Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"trainy_df = pd.DataFrame({\"target\":trainy.to_list()})\nclass_imbalance(trainy_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:07:15.322087Z","iopub.execute_input":"2022-06-11T12:07:15.322614Z","iopub.status.idle":"2022-06-11T12:07:16.622896Z","shell.execute_reply.started":"2022-06-11T12:07:15.322572Z","shell.execute_reply":"2022-06-11T12:07:16.621486Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"%%time\nplt.figure(figsize=(30,30))\ncol_name = train_x.columns.to_list()\n#numeric_cols = list(set(col_name)-set(discard))\ncorr = trainx[numeric_cols].corr()\nmatrix_mask=np.triu(corr)\nsb.heatmap(corr,annot=True,fmt=\"0.1g\",cmap=\"viridis\",mask=matrix_mask)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:07:22.756811Z","iopub.execute_input":"2022-06-11T12:07:22.757238Z","iopub.status.idle":"2022-06-11T12:08:03.437485Z","shell.execute_reply.started":"2022-06-11T12:07:22.757206Z","shell.execute_reply":"2022-06-11T12:08:03.436023Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Highy correlated columns should be dropped. ","metadata":{}},{"cell_type":"code","source":"#update numeric columns\ndrop_numeric_cols=[]\npair=[]\nfor col in numeric_cols:\n    for i in range(len(corr)):\n        if abs(corr[col].iloc[i]) >= 0.9 and col != numeric_cols[i] :\n            print(f\"{col} and {numeric_cols[i]} are highly correlated...\") \n            if col not in pair:\n                pair.append(col)\n                pair.append(numeric_cols[i])\n                drop_numeric_cols.append(col) \nnumeric_cols = list(set(numeric_cols)-set(drop_numeric_cols))\nprint(f\"Dropping columns : {drop_numeric_cols}\")\ndel drop_numeric_cols\ndel pair\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:08:17.636915Z","iopub.execute_input":"2022-06-11T12:08:17.637387Z","iopub.status.idle":"2022-06-11T12:08:17.984987Z","shell.execute_reply.started":"2022-06-11T12:08:17.637353Z","shell.execute_reply":"2022-06-11T12:08:17.983800Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"%%time\nfig,axes=plt.subplots(int(len(numeric_cols)/5),5)\nnrow=0\nncol=0\ndf = trainx.sample(1000)\nfor col in numeric_cols:\n    if ncol <5:\n       fig.set_figheight(10)\n       fig.set_figwidth(10)\n       g=sb.distplot(df[col],hist=True,kde=True,ax=axes[nrow,ncol])\n       g.set(title=col)\n       g.set(xlabel=None)\n       g.set(ylabel=None)\n        \n       ncol +=1\n    else:\n        nrow +=1\n        ncol=0\nfig.subplots_adjust(hspace=1.5) \nfig.subplots_adjust(wspace=1) \nplt.show()\ndel df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:08:22.493811Z","iopub.execute_input":"2022-06-11T12:08:22.494255Z","iopub.status.idle":"2022-06-11T12:08:35.601203Z","shell.execute_reply.started":"2022-06-11T12:08:22.494223Z","shell.execute_reply":"2022-06-11T12:08:35.599853Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"%%time\ndf= trainx.sample(1000)\nfor col1,col2 in zip(numeric_cols,numeric_cols[1:]):\n    if col1.startswith(\"S\") and col2.startswith(\"R\"):\n        plt.figure(figsize=(5,5))\n        sb.jointplot(x=col1,y=col2,data=df,kind=\"kde\")\n        plt.show()\ndel df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:08:42.534921Z","iopub.execute_input":"2022-06-11T12:08:42.535337Z","iopub.status.idle":"2022-06-11T12:08:48.137066Z","shell.execute_reply.started":"2022-06-11T12:08:42.535305Z","shell.execute_reply":"2022-06-11T12:08:48.135916Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"There are different approaches to detect outliers. Isolation Forest is one of them. I have used an alternative approach instead of using Isolation Forest for the large dataset.","metadata":{}},{"cell_type":"code","source":"#Detection of Outliers\nfrom sklearn.ensemble import IsolationForest\ndef detect_outliers(train_data,col):\n    cf = IsolationForest(random_state=224,n_jobs=-1).fit(np.array(train_data[col].to_list()).reshape(-1,1))\n    predict = cf.predict(np.array(train_data[col].to_list()).reshape(-1,1))\n    colors={1:\"blue\",-1:\"black\"}\n    df = pd.DataFrame({\"Colors\":predict})\n    plt.figure(figsize=(5,5))\n    train_data[col].plot(style='.',color=df[\"Colors\"].map(colors),alpha=0.6)\n    plt.title(f\"Outlier Detection of {col} \")\n    plt.show()\n    del df\n    gc.collect()\n    return predict","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:10:18.270434Z","iopub.execute_input":"2022-06-11T12:10:18.270950Z","iopub.status.idle":"2022-06-11T12:10:18.660506Z","shell.execute_reply.started":"2022-06-11T12:10:18.270914Z","shell.execute_reply":"2022-06-11T12:10:18.659115Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"#Imputation\ndef data_impute(data,col):\n    q1 = data[col].quantile(0.25)\n    q3 = data[col].quantile(0.75)\n    IQR = q3-q1\n    rng = 3*IQR\n    data[col]=np.where(data[col] >= q3+rng,data[col].median(),data[col])\n    data[col]=np.where(data[col] <= q1-rng,data[col].median(),data[col])","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:10:23.945470Z","iopub.execute_input":"2022-06-11T12:10:23.946288Z","iopub.status.idle":"2022-06-11T12:10:23.953468Z","shell.execute_reply.started":"2022-06-11T12:10:23.946251Z","shell.execute_reply":"2022-06-11T12:10:23.952240Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#Impute Numerical Values:\nfor col in numeric_cols:\n    data_impute(trainx,col)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:10:29.651930Z","iopub.execute_input":"2022-06-11T12:10:29.652366Z","iopub.status.idle":"2022-06-11T12:10:47.849464Z","shell.execute_reply.started":"2022-06-11T12:10:29.652332Z","shell.execute_reply":"2022-06-11T12:10:47.848419Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#Feature Engineering\ntrainx[\"Days\"]=train_x[\"S_2\"].dt.day\ntrainx[\"Month\"]=train_x[\"S_2\"].dt.month\ntrainx[\"Year\"]=train_x[\"S_2\"].dt.year","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:10:51.674070Z","iopub.execute_input":"2022-06-11T12:10:51.675042Z","iopub.status.idle":"2022-06-11T12:10:53.591168Z","shell.execute_reply.started":"2022-06-11T12:10:51.675001Z","shell.execute_reply":"2022-06-11T12:10:53.590049Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"customer_id = [ i for i in trainx[\"customer_ID\"].to_list()]\ntrainx.drop(\"customer_ID\",axis=1,inplace=True)\ntrainx.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:10:56.242993Z","iopub.execute_input":"2022-06-11T12:10:56.243449Z","iopub.status.idle":"2022-06-11T12:10:57.896298Z","shell.execute_reply.started":"2022-06-11T12:10:56.243415Z","shell.execute_reply":"2022-06-11T12:10:57.895274Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#Scale Numerical Values\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nscaler = StandardScaler()\nnum_cols = list(set(trainx.columns)-set(discard))\nscaler.fit(trainx[num_cols])\ntrainx_scaled = scaler.transform(trainx[num_cols])\npca = PCA()\ncomp = pca.fit(trainx_scaled)\nplt.plot(np.cumsum(comp.explained_variance_ratio_), color=\"green\")\nplt.grid(axis=\"both\")\nplt.xlabel(\"PRINCIPAL COMPONENTS\")\nplt.ylabel(\"VARIANCE\")\nsb.despine()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T12:11:39.996918Z","iopub.execute_input":"2022-06-11T12:11:39.997377Z","iopub.status.idle":"2022-06-11T12:12:51.497040Z","shell.execute_reply.started":"2022-06-11T12:11:39.997341Z","shell.execute_reply":"2022-06-11T12:12:51.495981Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"The first 60 components explains 80% of variation and the first 80 components explains more than 95% variation.\n\n\nWork is going on.Going to add more work related feature engineering. Thanks to **AMEX.**","metadata":{}}]}